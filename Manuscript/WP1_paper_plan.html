<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="R. Fournier, P. Downing and R.Ramsey">

<title>WP1_paper_plan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="WP1_paper_plan_files/libs/clipboard/clipboard.min.js"></script>
<script src="WP1_paper_plan_files/libs/quarto-html/quarto.js"></script>
<script src="WP1_paper_plan_files/libs/quarto-html/popper.min.js"></script>
<script src="WP1_paper_plan_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="WP1_paper_plan_files/libs/quarto-html/anchor.min.js"></script>
<link href="WP1_paper_plan_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="WP1_paper_plan_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="WP1_paper_plan_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="WP1_paper_plan_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="WP1_paper_plan_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">WP1_paper_plan</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>R. Fournier, P. Downing and R.Ramsey </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level1">
<h1>Abstract</h1>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li><p>The ability to quickly recognize a familiar face is very important because ()</p></li>
<li><p>Unsurprisingly, the face recognition process has been intensively investigated over the last 50 years (<span class="citation" data-cites="johnston2009">Johnston and Edmonds (<a href="#ref-johnston2009" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="natu2011">Natu and O’Toole (<a href="#ref-natu2011" role="doc-biblioref">2011</a>)</span>) and several attempts to model face recognition have been made (<span class="citation" data-cites="haxby2000">Haxby, Hoffman, and Gobbini (<a href="#ref-haxby2000" role="doc-biblioref">2000</a>)</span>) One of the most influential model is the functional model of face recognition by <span class="citation" data-cites="bruce1986">Bruce and Young (<a href="#ref-bruce1986" role="doc-biblioref">1986</a>)</span>. Very quick description of the model and putting the emphasis on three components: face recognition units (visual familiarity), person identity nodes (conceptual, semantic familiarity) and “cognitive system’ component.</p></li>
<li><p>Face recognition units (visual familiarity): literature on the effect of visual familiarity on the face recognition ability/performance <span class="citation" data-cites="clutterbuck2005">Clutterbuck and Johnston (<a href="#ref-clutterbuck2005" role="doc-biblioref">2005</a>)</span>, <span class="citation" data-cites="dwyer2009">Dwyer and Vladeanu (<a href="#ref-dwyer2009" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="hahn2017">Hahn and O’Toole (<a href="#ref-hahn2017" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="klatzky1984">Klatzky and Forrest (<a href="#ref-klatzky1984" role="doc-biblioref">1984</a>)</span>,</p></li>
<li><p>Person identity nodes (conceptual, semantic familiarity): literature on the effect of conceptual and semantic familiarity on the face recognition ability/performance <span class="citation" data-cites="akan2023">Akan and Benjamin (<a href="#ref-akan2023" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="bird2011">Bird et al. (<a href="#ref-bird2011" role="doc-biblioref">2011</a>)</span>, <span class="citation" data-cites="schwaninger2002">Schwaninger, Lobmaier, and Collishaw (<a href="#ref-schwaninger2002" role="doc-biblioref">2002</a>)</span></p></li>
<li><p>However, and like Brice and Young themselves mention, the face recognition process is supervised by a “cognitive system” component, processing and comparing information from the face recognition units and the person identity nodes (attention, associative memory and decision-making processes). However, impossible to investigate these processes with typical analysis and response times and accuracy. Using computational approach to gain new insights on this “cognitive system” component. <span class="citation" data-cites="hackel2018">Hackel and Amodio (<a href="#ref-hackel2018" role="doc-biblioref">2018</a>)</span>,</p></li>
<li><p>Evidence-accumulation models are a class of models which… translate the distribution of response times for correct and incorrect responses into latent psychological variables. Able to make inference about latent variable of interest (drift rate and threshold). Few examples of past successes (<span class="citation" data-cites="parker2023">Parker and Ramsey (<a href="#ref-parker2023" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="axt2021">Axt and Johnson (<a href="#ref-axt2021" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="ratcliff2001">Ratcliff, Thapar, and McKoon (<a href="#ref-ratcliff2001" role="doc-biblioref">2001</a>)</span>)</p></li>
<li><p>Two experiments to assess the effect of familiarity (visual familiarity and visual + perceptual familiarity) on parameters of the LBA model. More specifically… Developing hypotheses here.</p></li>
</ul>
</section>
<section id="experiment-1" class="level1">
<h1>Experiment 1</h1>
<section id="method-and-materials" class="level2">
<h2 class="anchored" data-anchor-id="method-and-materials">Method and Materials</h2>
<section id="pre-registration-and-open-science" class="level3">
<h3 class="anchored" data-anchor-id="pre-registration-and-open-science">Pre-registration and open science</h3>
<p>The research question, hypotheses, experimental design, planned analysis, and exclusion criteria were preregistered (link). All raw data, stimuli, data wrangling and analysis code are available on the open science framework (see link).</p>
</section>
<section id="participants" class="level3">
<h3 class="anchored" data-anchor-id="participants">Participants</h3>
<p>All participant were recruited using either an online recruiting platform (<a href="https://marktplatz.uzhalumni.ch" class="uri">https://marktplatz.uzhalumni.ch</a>) or via a mailing list dedicated to students from the Department of Psychology of the University of Zurich. Sample size was determined in advance. Following previous suggestions from <span class="citation" data-cites="heathcote2019">Heathcote et al. (<a href="#ref-heathcote2019" role="doc-biblioref">2019</a>)</span>, a parameter recovery study was run to estimate if 30 usable participant datasets were sufficient to accurately recover the parameter estimates. The parameter recovery study included 30 participant datasets, each with 180 trials (90 trials “familiar” and 90 trials “unfamiliar”) and with our selected model parameterisation. The data-generated parameters were sufficiently well-recovered (See Supplementary Materials).</p>
<p>31 healthy participants took part in experiment 2 (11 men and 20 women). Ages ranged from 20 to 43 years old (<span class="math inline">\(M_{age} =\)</span> 28.29, <span class="math inline">\(SD_{age} =\)</span> 5.45). Participants were recruited by . All participants reported normal or corrected-to-normal vision and received payment (20 CHF) for participating in the experiment. Based on our preregistration, response times longer than 2.5 seconds were removed from the analysis. However, this cutoff value led to the the removal of 59% of the trials of one participant. Given the recommendation of having a certain number of trials per condition per participant when using evidence-accumulation models <span class="citation" data-cites="heathcote2019">Heathcote et al. (<a href="#ref-heathcote2019" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="parker">Parker and Ramsey (<a href="#ref-parker" role="doc-biblioref">n.d.</a>)</span> we decided to exclude this participant from the data analysis. This experiment was accepted by the ethics committee of the Federal Institute of Technology Zurich (ETHZ), and all participant gave their informed consent to participate in the study.</p>
</section>
<section id="stimuli" class="level3">
<h3 class="anchored" data-anchor-id="stimuli">Stimuli</h3>
<p>The stimuli consisted of images selected from two different data sets of face images: the Face Research Lab London Set (DeBruine, Lisa; Jones, Benedict (2017). Face Research Lab London Set. figshare. Data set. <a href="https://doi.org/10.6084/m9.figshare.5047666.v5" class="uri">https://doi.org/10.6084/m9.figshare.5047666.v5</a>) and the (other datas et). Each of these data sets is composed of images of different identities faces taken from different angle points. (X) identities were selected from the Face Research Lab London Set and (X) from the . For each selected identity, three pictures of the face at three different view points (3/4 left view, frontal view and 3/4 right view) were cropped to remove any (Fig. 1). All of the images processing was realized using the GNU Image Manipulation Program (GIMP, ref). After 4 pilots experiments, we removed 27 identities (21 from the Face Research Lab London Set and 6 from the blablabla) due to to the presence of (blablabla). In total, the data set of face images used in the main experiment contained 756 images: one set of three colorful face images and one set of three gray scaled face images for each of the 126 identities.</p>
</section>
<section id="material" class="level3">
<h3 class="anchored" data-anchor-id="material">Material</h3>
<p>Stimuli were presented on the screen of MacBook (blabla). The experiment was run on Psychopy Coder (version: 2022.5, ref).</p>
</section>
<section id="procedure" class="level3">
<h3 class="anchored" data-anchor-id="procedure">Procedure</h3>
<p>The experimental task was divided into two distinct parts: a familiarization part and a recognition part (Fig.2). In the familiarization part, the face of 30 unknown identities were presented on the screen and participants were told to memorize the identity of the faces. One familiarization trial consisted of the presentation of 6 face images, one after the other, depicting the same identity but varying the view points (3/4 left view, frontal view and 3/4 right view) and the colorimetry (colorful image, gray scale image). The duration of each image presentation was 0.75 second. Each participant underwent 30 familiarization trials. During the familiarization, we introduced catch trials to ensure participants attention. After some familiarization trials, a gray scale image of a person’s face was presented on the screen. Participants were asked to press the “q” key if the face belonged to the last seen identity or the “p” key if it was not. The faces seen during the familiarization were considered the “Learned faces” within-subject condition. In the recognition part, participants completed three blocks of 90 recognition trials. Each block was separated from another by a short break. Half of the trials were learned identities trials and the other half were new, previously unseen identities trials. At each trial, a gray scale image of a face was shown on the screen. Participants were asked to choose if they had previously seen this face in the familiarization part or not. If they did, the should press the “q” key and if they did not, the “p” key. Response time [s] and accuracy were collected after each recognition trial.</p>
</section>
<section id="data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="data-analysis">Data analysis</h3>
<p>Following our preregistration, trials with a response time longer than 2.5s were removed from the analysis. The evidence-accumulation modelling analyses of the distributions of response times for the correct and wrong answers were realized using the EMC2 R package (Cit. Niek Stevenson). For the separate analysis of response times and accuracy, we used (blablabla).</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
</section>
</section>
<section id="experiment-2" class="level1">
<h1>Experiment 2</h1>
<section id="method-and-materials-1" class="level2">
<h2 class="anchored" data-anchor-id="method-and-materials-1">Method and Materials</h2>
<section id="pre-registration-and-open-science-1" class="level3">
<h3 class="anchored" data-anchor-id="pre-registration-and-open-science-1">Pre-registration and open science</h3>
<p>The research question, hypotheses, experimental design, planned analysis, and exclusion criteria were preregistered (<a href="https://osf.io/xehf4" class="uri">https://osf.io/xehf4</a>). All raw data, stimuli, data wrangling and analysis code are available on the open science framework (see link).</p>
</section>
<section id="participants-1" class="level3">
<h3 class="anchored" data-anchor-id="participants-1">Participants</h3>
<p>All participant were recruited using either an online recruiting platform (<a href="https://marktplatz.uzhalumni.ch" class="uri">https://marktplatz.uzhalumni.ch</a>) or via a mailing list dedicated to students from the Department of Psychology of the University of Zurich. Sample size was determined in advance. Following previous suggestions from <span class="citation" data-cites="heathcote2019">Heathcote et al. (<a href="#ref-heathcote2019" role="doc-biblioref">2019</a>)</span>, a parameter recovery study was run to estimate if 30 usable participant datasets were sufficient to accurately recover the parameter estimates. The parameter recovery study included 30 participant datasets, each with 180 trials (90 trials “familiar” and 90 trials “unfamiliar”) and with our selected model parameterisation. The data-generated parameters were sufficiently well-recovered (See Supplementary Materials).</p>
<p>38 healthy participants took part in experiment 2 (16 men and 22 women). Ages ranged from 20 to 48 years old (<span class="math inline">\(M_{age} =\)</span> 27.16, <span class="math inline">\(SD_{age} =\)</span> 6.35). Participants were recruited by . All participants reported normal or corrected-to-normal vision and received payment (25 CHF) for participating in the experiment. Based on our preregistration criterion, 3 participants were excluded from the data analysis: 1 of them had an overall recognition accuracy larger than 95% and 2 of them had a overall accuracy lower than 55% (chance at 50%). We deviated from our preregistration in… This experiment was accepted by the ethics committee of the Federal Institute of Technology Zurich (ETHZ), and all participant gave their informed consent to participate in the study.</p>
</section>
<section id="stimuli-1" class="level3">
<h3 class="anchored" data-anchor-id="stimuli-1">Stimuli</h3>
<p>Two types of pictures were used for this experiment to create our within-subject stimuli conditions. Pictures depicting celebrities were taken from the internet The stimuli consisted of images selected from two different data sets of face images: the Face Research Lab London Set (DeBruine, Lisa; Jones, Benedict (2017). Face Research Lab London Set. figshare. Data set. <a href="https://doi.org/10.6084/m9.figshare.5047666.v5" class="uri">https://doi.org/10.6084/m9.figshare.5047666.v5</a>) and the (other datas et). Each of these data sets is composed of images of different identities faces taken from different angle points. (X) identities were selected from the Face Research Lab London Set and (X) from the . For each selected identity, three pictures of the face at three different view points (3/4 left view, frontal view and 3/4 right view) were cropped to remove any (Fig. 1). All of the images processing was realized using the GNU Image Manipulation Program (GIMP, ref). After 4 pilots experiments, we removed 27 identities (21 from the Face Research Lab London Set and 6 from the blablabla) due to to the presence of (blablabla). In total, the data set of face images used in the main experiment contained 756 images: one set of three colorful face images and one set of three gray scaled face images for each of the 126 identities.</p>
</section>
<section id="material-1" class="level3">
<h3 class="anchored" data-anchor-id="material-1">Material</h3>
<p>Stimuli were presented on the screen of MacBook (blabla). The experiment was run on Psychopy Coder (version: 2022.5, ref).</p>
</section>
<section id="procedure-1" class="level3">
<h3 class="anchored" data-anchor-id="procedure-1">Procedure</h3>
<p>The experimental task was divided into two distinct parts: a rating part and a recognition part (Fig.3). During the rating part, participants were presented pictures of 60 celebrities. They were asked to rate these celebrities, based on two dimensions: how familiar the celebrity was to them and how positive or negative were their feelings towards the celebrity. The familiarity rating scale went from 0 (Highly unfamiliar) to 10 (Highly familiar).</p>
<p>In the recognition part, participants completed three blocks of 90 recognition trials. Each block was separated from another by a short break. Half of the trials were famous identities trials and the other half were new, previously unseen identities trials. At each trial, a gray scale image of a face was shown on the screen. Participants were asked to choose if they recognize the face (and not the picture) seen this face in the familiarization part or not. If they did, the should press the “q” key and if they did not, the “p” key. Response time [s] and accuracy were collected after each trial. Time limit</p>
</section>
<section id="data-analysis-1" class="level3">
<h3 class="anchored" data-anchor-id="data-analysis-1">Data analysis</h3>
<p>The evidence-accumulation modelling analyses of the distribution of response times for the correct and wrong answers were realized using the EMC2 R package (Cit. Niek Stevenson). For the separate analysis of response times and accuracy, we used (blablabla).</p>
</section>
</section>
<section id="results-1" class="level2">
<h2 class="anchored" data-anchor-id="results-1">Results</h2>
</section>
<section id="discussion-1" class="level2">
<h2 class="anchored" data-anchor-id="discussion-1">Discussion</h2>
</section>
</section>
<section id="general-discussion" class="level1">
<h1>General discussion</h1>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
</section>
<section id="bibliography" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-akan2023" class="csl-entry" role="listitem">
Akan, Melisa, and Aaron S. Benjamin. 2023. <span>“Haven<span>’</span>t I Seen You Before? Conceptual but Not Perceptual Prior Familiarity Enhances Face Recognition Memory.”</span> <em>Journal of Memory and Language</em> 131 (August): 104433. <a href="https://doi.org/10.1016/j.jml.2023.104433">https://doi.org/10.1016/j.jml.2023.104433</a>.
</div>
<div id="ref-axt2021" class="csl-entry" role="listitem">
Axt, Jordan R., and David J. Johnson. 2021. <span>“Understanding Mechanisms Behind Discrimination Using Diffusion Decision Modeling.”</span> <em>Journal of Experimental Social Psychology</em> 95 (July): 104134. <a href="https://doi.org/10.1016/j.jesp.2021.104134">https://doi.org/10.1016/j.jesp.2021.104134</a>.
</div>
<div id="ref-bird2011" class="csl-entry" role="listitem">
Bird, Chris M., Rachel A. Davies, Jamie Ward, and Neil Burgess. 2011. <span>“Effects of Pre-Experimental Knowledge on Recognition Memory.”</span> <em>Learning &amp; Memory</em> 18 (1): 11–14. <a href="https://doi.org/10.1101/lm.1952111">https://doi.org/10.1101/lm.1952111</a>.
</div>
<div id="ref-bruce1986" class="csl-entry" role="listitem">
Bruce, Vicki, and Andy Young. 1986. <span>“Understanding Face Recognition.”</span> <em>British Journal of Psychology</em> 77 (3): 305–27. <a href="https://doi.org/10.1111/j.2044-8295.1986.tb02199.x">https://doi.org/10.1111/j.2044-8295.1986.tb02199.x</a>.
</div>
<div id="ref-clutterbuck2005" class="csl-entry" role="listitem">
Clutterbuck, R., and R. A. Johnston. 2005. <span>“Demonstrating How Unfamiliar Faces Become Familiar Using a Face Matching Task.”</span> <em>European Journal of Cognitive Psychology</em> 17 (1): 97–116. <a href="https://doi.org/10.1080/09541440340000439">https://doi.org/10.1080/09541440340000439</a>.
</div>
<div id="ref-dwyer2009" class="csl-entry" role="listitem">
Dwyer, Dominic M., and Matei Vladeanu. 2009. <span>“Perceptual Learning in Face Processing: Comparison Facilitates Face Recognition.”</span> <em>Quarterly Journal of Experimental Psychology</em> 62 (10): 2055–67. <a href="https://doi.org/10.1080/17470210802661736">https://doi.org/10.1080/17470210802661736</a>.
</div>
<div id="ref-hackel2018" class="csl-entry" role="listitem">
Hackel, Leor M, and David M Amodio. 2018. <span>“Computational Neuroscience Approaches to Social Cognition.”</span> <em>Current Opinion in Psychology</em>, Social neuroscience, 24 (December): 92–97. <a href="https://doi.org/10.1016/j.copsyc.2018.09.001">https://doi.org/10.1016/j.copsyc.2018.09.001</a>.
</div>
<div id="ref-hahn2017" class="csl-entry" role="listitem">
Hahn, Carina A., and Alice J. O’Toole. 2017. <span>“Recognizing Approaching Walkers: Neural Decoding of Person Familiarity in Cortical Areas Responsive to Faces, Bodies, and Biological Motion.”</span> <em>NeuroImage</em> 146 (February): 859–68. <a href="https://doi.org/10.1016/j.neuroimage.2016.10.042">https://doi.org/10.1016/j.neuroimage.2016.10.042</a>.
</div>
<div id="ref-haxby2000" class="csl-entry" role="listitem">
Haxby, James V., Elizabeth A. Hoffman, and M.Ida Gobbini. 2000. <span>“The Distributed Human Neural System for Face Perception.”</span> <em>Trends in Cognitive Sciences</em> 4 (6): 223–33. <a href="https://doi.org/10.1016/S1364-6613(00)01482-0">https://doi.org/10.1016/S1364-6613(00)01482-0</a>.
</div>
<div id="ref-heathcote2019" class="csl-entry" role="listitem">
Heathcote, Andrew, Yi-Shin Lin, Angus Reynolds, Luke Strickland, Matthew Gretton, and Dora Matzke. 2019. <span>“Dynamic Models of Choice.”</span> <em>Behavior Research Methods</em> 51 (2): 961–85. <a href="https://doi.org/10.3758/s13428-018-1067-y">https://doi.org/10.3758/s13428-018-1067-y</a>.
</div>
<div id="ref-johnston2009" class="csl-entry" role="listitem">
Johnston, Robert A., and Andrew J. Edmonds. 2009. <span>“Familiar and Unfamiliar Face Recognition: A Review.”</span> <em>Memory</em> 17 (5): 577–96. <a href="https://doi.org/10.1080/09658210902976969">https://doi.org/10.1080/09658210902976969</a>.
</div>
<div id="ref-klatzky1984" class="csl-entry" role="listitem">
Klatzky, Roberta L., and Fiona H. Forrest. 1984. <span>“Recognizing Familiar and Unfamiliar Faces.”</span> <em>Memory &amp; Cognition</em> 12 (1): 60–70. <a href="https://doi.org/10.3758/BF03196998">https://doi.org/10.3758/BF03196998</a>.
</div>
<div id="ref-natu2011" class="csl-entry" role="listitem">
Natu, Vaidehi, and Alice J. O’Toole. 2011. <span>“The Neural Processing of Familiar and Unfamiliar Faces: A Review and Synopsis.”</span> <em>British Journal of Psychology</em> 102 (4): 726–47. <a href="https://doi.org/10.1111/j.2044-8295.2011.02053.x">https://doi.org/10.1111/j.2044-8295.2011.02053.x</a>.
</div>
<div id="ref-parker2023" class="csl-entry" role="listitem">
Parker, Samantha, and Richard Ramsey. 2023. <span>“Exploring the Relationship Between Oculomotor Preparation and Gaze-Cued Covert Shifts in Attention.”</span> <em>Journal of Vision</em> 23 (3): 18. <a href="https://doi.org/10.1167/jov.23.3.18">https://doi.org/10.1167/jov.23.3.18</a>.
</div>
<div id="ref-parker" class="csl-entry" role="listitem">
———. n.d. <span>“What Can Evidence Accumulation Modelling Tell Us about Human Social Cognition?”</span> <em>Quarterly Journal of Experimental Psychology</em>.
</div>
<div id="ref-ratcliff2001" class="csl-entry" role="listitem">
Ratcliff, Roger, Anjali Thapar, and Gail McKoon. 2001. <span>“The Effects of Aging on Reaction Time in a Signal Detection Task.”</span> <em>Psychology and Aging</em> 16 (2): 323–41. <a href="https://doi.org/10.1037/0882-7974.16.2.323">https://doi.org/10.1037/0882-7974.16.2.323</a>.
</div>
<div id="ref-schwaninger2002" class="csl-entry" role="listitem">
Schwaninger, Adrian, Janek S. Lobmaier, and Stephan M. Collishaw. 2002. <span>“Role of Featural and Configural Information in Familiar and Unfamiliar Face Recognition.”</span> In, edited by Heinrich H. Bülthoff, Christian Wallraven, Seong-Whan Lee, and Tomaso A. Poggio, 2525:643–50. Berlin, Heidelberg: Springer Berlin Heidelberg. <a href="http://link.springer.com/10.1007/3-540-36181-2_64">http://link.springer.com/10.1007/3-540-36181-2_64</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>