<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphaël Fournier, Paul Downing and Richard Ramsey">

<title>Test_quarto</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Quarto_test_files/libs/clipboard/clipboard.min.js"></script>
<script src="Quarto_test_files/libs/quarto-html/quarto.js"></script>
<script src="Quarto_test_files/libs/quarto-html/popper.min.js"></script>
<script src="Quarto_test_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Quarto_test_files/libs/quarto-html/anchor.min.js"></script>
<link href="Quarto_test_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Quarto_test_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Quarto_test_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Quarto_test_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Quarto_test_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Test_quarto</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Raphaël Fournier, Paul Downing and Richard Ramsey </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The ability to quickly recognize a familiar face is an important adaptive skill, especially when considering the number of social interactions a typical human being (has) during its entire life. Not only person identification allow us to anticipate another person behavior but we will adapt our own response in function of the identity the person (cit.). Thus, it would not be exaggerated to stipulate that identity recognition is one of the first (earliest) essential features that shapes social interactions.</p>
<p>Unsurprisingly, the face recognition process has been intensively investigated over the last 50 years (citation review literature face recognition).</p>
<p>One key factor affecting the ability to recognize a face is level of familiarity towards the face. Familiarity, in the context of recognition memory, has been defined as the “quality or depth of encoding during a critical study context”<span class="citation" data-cites="akan2023a">(<a href="#ref-akan2023a" role="doc-biblioref">Akan and Benjamin 2023a</a>)</span>.</p>
<p>Using a face matching paradigm, several studies have shown a clear performance advantage (i.e a higher accuracy and lower response times) when participants had to match familiar faces compared to unfamiliar ones (cit.).</p>
<p>The most distinctive feature for identity recognition: face. Since the 1970’s the face recognition process has been extensively investigated</p>
<p>One key factor affecting our ability to recognize someone is our subjective level of familiarity towards this person. - Differences between familiar versus unfamiliar Familiar and Unfamiliar face recognition: A review (Johnston &amp; Edmonds, 2009) Internal vs external features: for unfamiliar faces, the type of features is approximately of equal importance for recognition, although advantages have been found for the external features in some studies(Bruce et al., 1999) Masking the each region had the greated detrimental effect on recognition performance compared with other face areas (Robert and Bruce, 1988). However, Burton et al.&nbsp;(2005) propose that what occurs as a face becomes familiar goes beyond the simple summation of encounters with a face. Encountering a face from images taken with such a range of different cameras, with a wide range of viewpoints, lighting conditions etc., enables us to eliminate the properties of the image that are not diagnostic of identity. When a face is unfamiliar, we are unable to know which characteristics or image properties will be key to representing the identity of an individual, forcing us into a primitive image-matching strategy with these faces. Distinction between a continuum of meaningfulness ranging from the unfamiliar (once viewed) face to the extremely familiar face (Rhodes, 1985) and qualitatively distinct kinds of associative coding, only some of which are available for the unfamiliar face.</p>
</section>
<section id="model-of-face-recognition-perception" class="level2">
<h2 class="anchored" data-anchor-id="model-of-face-recognition-perception">Model of face recognition-perception</h2>
<ul>
<li>Bruce and Young, 1986 Functional model of face recognition Theoretical framework for face recognition which draws together and extends recent models. Contrast recognition of people’s faces with recognition of other types of visual stimuli. Present a functional model to account for the perceptual and cognitive processes involved when people recognizing faces. Recognition of familiar faces involves structural, identity-specific semantic and name codes. Minor role are pictorial, expression and facial speech codes. A face can be recognized as familiar when there is a match between its encoded representation and a stored structural code. Bruce and Young ephasized a distinction between processes involved in the recognition of identity and the recognition of expression and speech-related movements of the mouth. The anatomical organization of face-responsive regions in extrastriate vosual cortex provides a substrate that embodies this cognitive distinction.</li>
</ul>
<p>Model: Functional components in the human face processing system:</p>
<ul>
<li>Haxby et al.&nbsp;(2000) model of face perception: distributed human neural system for face perception</li>
</ul>
<ol type="1">
<li>Core system, exastriate visual cortex: visual analysis of faces</li>
<li>Extended system: further processing of the meaning of information gleaned from faces (regions parts of neural systems for other cognitive functions) Anterior temporal lobe (personal identity, name, biographical information): high functional connectivity with hippocampal formation (declarative memory). A model of the distributed human neural system for face perception (Haxby et al., 2000). The model is divided into a core system for the visual analysis of faces, which consists of three regions of occipitotemporal visual extrastriate cortex, and an extended system for further processing of the meaning of information gleaned from faces, which consists of regions that are also part of neural systems for other cognitive functions. Changeable and invariant aspects of the visual facial configuration have distinct representations in the core system. Interactions between these representations in the core system and regions in the extended system meditate processing of the spatial focus of another’s attention, speech-related mouth movements, facial expression, and identity. processing the emotional content of a face and the evocation of an emotional reponse to a face can be based on changeable aspects of the face, such as expression and eye gaze, or on identity and knowledge of the person being viewed.</li>
</ol>
<p>Perception of identity and retrieval of semantic knowledge about people: a novel face is perceived as a unique individual even when one has no other knowledge of that person, and this perception of the unique identity of a face appears to be associated with activity in lateral fusiform gyrus (George et al., 1999; Hoffman and Haxby, 2000; Sergent et al, 1992). The recognition of a familiar face appears to involve a fixed sequence of stages in which the retrieval of semantic information about a person precedes the retrieval of that person’s name (Ellis, 1992). Recognition of familiar faces appears to be associated with activity in anterior temporal regions (Gorno Tempini et al 1988; Leveroni et al 2000; Nakamura et al 2000; Sergent et al 1992), especially the anterior temporal gyrus. Activity in this region is allso elicited by the perception of the names of famous people and by the perception of familiar outdoor scenes (Gorno Temponi et al., 1998; Nakamura et al., 2000), suggesting that this region may be associated with the representation of biographical knowledge (CC.)</p>
<p>Common point regarding face recognition: comparion between</p>
<ul>
<li>Issues/Gap: Face matching task: familiar faces are recognized faster and more precisely than unfamiliar or new faces. Real world: not a face matching task. See a face, recognize or not. Recognition memory (face memory task). In this case, the clear advantage from familiarity is not as evident. Several studies found advantage (ref) while others found no advantage or even a disadvantage (ref). Unfamiliar (learned faces) vs highly familiar vs new faces: 3 different levels of familiarity. Look at effect of familiarity cognition behind this face recognition process using evidence-accumulation approach. Advantage: combination/translation of RT and accuracy into latent variables assumed to underlie the decision-making process (not just faster, better, but why?) /</li>
</ul>
<p>Limitations/Gap/Issues:</p>
</section>
<section id="evidence-accumulation-models" class="level2">
<h2 class="anchored" data-anchor-id="evidence-accumulation-models">Evidence accumulation models</h2>
<ul>
<li>Definition: model describing two-forced choice accumulation of evidence. Evidence-accumulation models: most successful frameworks used to account for human decision-making (Donkin &amp; Brown, 2018). Idea as measurements tools: translation of RT and accuracy into latent variables assumed to underlie the decision-making process. Application (aging literature, covert attention, perceptual discrimination tasks…). Success within cognitive psychology, last years few attempts to use them to answer questions related to human social cognition (Parker &amp; Ramsey, 2023).</li>
<li>Linear Ballistic Model (LBA): race model, When someone is faced between two options, evidence accumulate in favor of these two options at a different speed (two distinct accumulators).</li>
</ul>
</section>
<section id="hypotheses" class="level2">
<h2 class="anchored" data-anchor-id="hypotheses">Hypotheses</h2>
</section>
<section id="experiment-1" class="level1">
<h1>Experiment 1</h1>
<section id="materials-and-methods" class="level2">
<h2 class="anchored" data-anchor-id="materials-and-methods">2. Materials and Methods</h2>
<p>We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) --></p>
<section id="pre-registration-and-open-science" class="level3">
<h3 class="anchored" data-anchor-id="pre-registration-and-open-science">Pre-registration and open science</h3>
<p>For both experiments 1 and 2, the research question, hypotheses, experimental design, planned analysis, and exclusion criteria were preregistered (for Experiment 1: ; for Experiment 2:). All raw data, stimuli, data wrangling and analysis code are avialble on the open science framework (see link). We did not deviated from our preregistered experimental plan.</p>
</section>
<section id="participants" class="level3">
<h3 class="anchored" data-anchor-id="participants">Participants</h3>
<p>We tested 31 healthy participants through blablabla (number of females, between the ages of blablabla). All participants reported normal or corrected-to-normal vision and received payment (20 CHF) for participating in the experiment. One participant was excluded from the data analysis (blablabla). This experiment was accepted by the ethics committee of the Federal Institute of Technology Zurich (ETHZ), and all participant gave their informed consent to participate in the study. Sample size was determined in advance…</p>
</section>
<section id="stimuli" class="level3">
<h3 class="anchored" data-anchor-id="stimuli">Stimuli</h3>
<p>The stimuli consisted of images selected from two different data sets of face images: the Face Research Lab London Set (DeBruine, Lisa; Jones, Benedict (2017). Face Research Lab London Set. figshare. Data set. https://doi.org/10.6084/m9.figshare.5047666.v5) and the (other datas et). Each of these data sets is composed of images of different identities faces taken from different angle points. (X) identities were selected from the Face Research Lab London Set and (X) from the . For each selected identity, three pictures of the face at three different view points (3/4 left view, frontal view and 3/4 right view) were cropped to remove any (Fig. 1). All of the images processing was realized using the GNU Image Manipulation Program (GIMP, ref). After 4 pilots experiments, we removed 27 identities (21 from the Face Research Lab London Set and 6 from the blablabla) due to to the presence of (blablabla). In total, the data set of face images used in the main experiment contained 756 images: one set of three colorful face images and one set of three gray scaled face images for each of the 126 identities.</p>
</section>
<section id="material" class="level3">
<h3 class="anchored" data-anchor-id="material">Material</h3>
<p>Stimuli were presented on the screen of MacBook (blabla). The experiment was run on Psychopy Coder (version: 2022.5, ref).</p>
</section>
<section id="procedure" class="level3">
<h3 class="anchored" data-anchor-id="procedure">Procedure</h3>
<p>The experimental task was divided into two distinct parts: a familiarization part and a recognition part (Fig.2). In the familiarization part, the face of 30 unknown identities were presented on the screen and participants were told to memorize the identity of the faces. One familiarization trial consisted of the presentation of 6 face images, one after the other, depicting the same identity but varying the view points (3/4 left view, frontal view and 3/4 right view) and the colorimetry (colorful image, gray scale image). The duration of each image presentation was 0.75 second. Each participant underwent 30 familiarization trials. The faces seen during the familiarization were considered the “Learned faces” condition. In the recognition part, participants completed three blocks of 90 recognition trials. Each block was separated from another by a short break. Half of the trials were learned identities trials and the other half were new, previously unseen identities trials. At each trial, a gray scale image of a face was shown on the screen. Participants were asked to choose if they had previously seen this face in the familiarization part or not. If they did, the should press the “q” key and if they did not, the “p” key. Response time [s] and accuracy were collected after each trial. During the familiarization, we introduced catch trials to ensure participants attention. After some familiarization trials, a gray scale image of a person’s face was presented on the screen. Participants were asked to press the “q” key if the face belonged to the last seen identity or the “p” key if it was not.</p>
</section>
<section id="data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="data-analysis">Data analysis</h3>
<p>Following our preregistration, trials with a response time longer than 2.5s were removed from the analysis. The evidence-accumulation modelling analyses of the distribution of response times for the correct and wrong answers were realized using the EMC2 R package (Cit. Niek Stevenson). For the separate analysis of response times and accuracy, we used (blablabla).</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">3. Results</h2>
<section id="analysis-of-observed-variables" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-observed-variables">Analysis of observed variables</h3>
<p>Prior to the evidence accumulation modeling analysis, we separately analyzed RT and accuracy as they were the variables directly measured in our task.</p>
</section>
<section id="evidence-accumulation-modelling" class="level3">
<h3 class="anchored" data-anchor-id="evidence-accumulation-modelling">Evidence accumulation modelling</h3>
<section id="model-specification" class="level4">
<h4 class="anchored" data-anchor-id="model-specification">Model specification</h4>
</section>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">4. Discussion</h2>
</section>
</section>
<section id="experiment-2" class="level1">
<h1>Experiment 2</h1>
<section id="materials-and-methods-1" class="level2">
<h2 class="anchored" data-anchor-id="materials-and-methods-1">2. Materials and Methods</h2>
<section id="participants-1" class="level3">
<h3 class="anchored" data-anchor-id="participants-1">Participants</h3>
<p>We tested 33 healthy participants through blablabla (number of females, between the ages of blablabla). All participants reported normal or corrected-to-normal vision and received payment (25 CHF) for participating in the experiment. Based on our preregistration criterion, 3 participants were excluded from the data analysi: 1 of them had an overall recognition accuracy larger than 95% and 2 of them had a overall accuracy lower than 55% (chance at 50%). This experiment was accepted by the ethics committee of the Federal Institute of Technology Zurich (ETHZ), and all participant gave their informed consent to participate in the study. Sample size was determined in advance…</p>
</section>
<section id="stimuli-1" class="level3">
<h3 class="anchored" data-anchor-id="stimuli-1">Stimuli</h3>
<p>Two types of pictures were used for this experiment to create our within-subject stimuli conditions. Pictures depicting celebrities were taken from the internet The stimuli consisted of images selected from two different data sets of face images: the Face Research Lab London Set (DeBruine, Lisa; Jones, Benedict (2017). Face Research Lab London Set. figshare. Data set. https://doi.org/10.6084/m9.figshare.5047666.v5) and the (other datas et). Each of these data sets is composed of images of different identities faces taken from different angle points. (X) identities were selected from the Face Research Lab London Set and (X) from the . For each selected identity, three pictures of the face at three different view points (3/4 left view, frontal view and 3/4 right view) were cropped to remove any (Fig. 1). All of the images processing was realized using the GNU Image Manipulation Program (GIMP, ref). After 4 pilots experiments, we removed 27 identities (21 from the Face Research Lab London Set and 6 from the blablabla) due to to the presence of (blablabla). In total, the data set of face images used in the main experiment contained 756 images: one set of three colorful face images and one set of three gray scaled face images for each of the 126 identities.</p>
</section>
<section id="material-1" class="level3">
<h3 class="anchored" data-anchor-id="material-1">Material</h3>
<p>Stimuli were presented on the screen of MacBook (blabla). The experiment was run on Psychopy Coder (version: 2022.5, ref).</p>
</section>
<section id="procedure-1" class="level3">
<h3 class="anchored" data-anchor-id="procedure-1">Procedure</h3>
<p>The experimental task was divided into two distinct parts: a rating part and a recognition part (Fig.2). During the rating part, participants were presented pictures of 60 celebrities. They were asked to rate these celebrities, based on two dimensions: how familiar the celebrity was to them and how positive or negative were their feelings towards the celebrity. The familiarity rating scale used went from 0 (Highly unfamiliar) to 10 (Highly familiar). and were asked to rate the , based on two dimensions: the level of familiarity of In the recognition part, participants completed three blocks of 90 recognition trials. Each block was separated from another by a short break. Half of the trials were famous identities trials and the other half were new, previously unseen identities trials. At each trial, a gray scale image of a face was shown on the screen. Participants were asked to choose if they recognize the face (and not the picture) seen this face in the familiarization part or not. If they did, the should press the “q” key and if they did not, the “p” key. Response time [s] and accuracy were collected after each trial.</p>
</section>
<section id="data-analysis-1" class="level3">
<h3 class="anchored" data-anchor-id="data-analysis-1">Data analysis</h3>
<p>Following our preregistration, trials with a response time longer than 2.5s were removed from the analysis. The evidence-accumulation modelling analyses of the distribution of response times for the correct and wrong answers were realized using the EMC2 R package (Cit. Niek Stevenson). For the separate analysis of response times and accuracy, we used (blablabla).</p>
</section>
</section>
<section id="results-1" class="level2">
<h2 class="anchored" data-anchor-id="results-1">3. Results</h2>
<section id="analysis-of-observed-variables-1" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-observed-variables-1">Analysis of observed variables</h3>
<p>Prior to the evidence accumulation modeling analysis, we separately analyzed RT and accuracy as they were the variables directly measured in our task.</p>
</section>
<section id="evidence-accumulation-modelling-1" class="level3">
<h3 class="anchored" data-anchor-id="evidence-accumulation-modelling-1">Evidence accumulation modelling</h3>
<section id="model-specification-1" class="level4">
<h4 class="anchored" data-anchor-id="model-specification-1">Model specification</h4>
</section>
</section>
</section>
<section id="discussion-1" class="level2">
<h2 class="anchored" data-anchor-id="discussion-1">4. Discussion</h2>
</section>
</section>
<section id="general-discussion" class="level1">
<h1>General discussion</h1>
<p>Let’s plot the palmerpenguins data.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="Quarto_test_files/figure-html/plot-penguins-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Testing Equations</p>
<p><span class="math display">\[
price = \hat{\beta}_0 + \hat{\beta}_1 \times area + \epsilon
\]</span></p>
<p>Citations</p>
<p>We are going to use <span class="citation" data-cites="akan2023">(<a href="#ref-akan2023" role="doc-biblioref">Akan and Benjamin 2023b</a>)</span> to analyse the data.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-akan2023" class="csl-entry" role="listitem">
Akan, Melisa, and Aaron S. Benjamin. 2023b. <span>“Haven<span>’</span>t I Seen You Before? Conceptual but Not Perceptual Prior Familiarity Enhances Face Recognition Memory.”</span> <em>Journal of Memory and Language</em> 131 (August): 104433. <a href="https://doi.org/10.1016/j.jml.2023.104433">https://doi.org/10.1016/j.jml.2023.104433</a>.
</div>
<div id="ref-akan2023a" class="csl-entry" role="listitem">
———. 2023a. <span>“Haven<span>’</span>t I Seen You Before? Conceptual but Not Perceptual Prior Familiarity Enhances Face Recognition Memory.”</span> <em>Journal of Memory and Language</em> 131 (August): 104433. <a href="https://doi.org/10.1016/j.jml.2023.104433">https://doi.org/10.1016/j.jml.2023.104433</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>