<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="WP_1_paper_files/libs/clipboard/clipboard.min.js"></script>
<script src="WP_1_paper_files/libs/quarto-html/quarto.js"></script>
<script src="WP_1_paper_files/libs/quarto-html/popper.min.js"></script>
<script src="WP_1_paper_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="WP_1_paper_files/libs/quarto-html/anchor.min.js"></script>
<link href="WP_1_paper_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="WP_1_paper_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="WP_1_paper_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="WP_1_paper_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="WP_1_paper_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>P1: Introduction paragraph The ability to quickly and correctly recognize a familiar face is important in our everyday life. Not only does this ability allow us to anticipate another person’s behaviour, but this recognition will influence our own behaviour as well. People will behave differently in function of the identity of the person they are interacting with. Most of the previous research focusing on face recognition/perception has used manifest measures (i.e.&nbsp;response times and accuracy) to assess one’s ability to recognize faces. However, less is known about the computational processes that support face recognition. We investigated how different types/qualities of face familiarity affected the latent variables estimated by evidence accumulation models (EAMs; <span class="citation" data-cites="ratcliff1998">Ratcliff and Rouder (<a href="#ref-ratcliff1998" role="doc-biblioref">1998</a>)</span>). This computational approach has provided new insights on the dynamics of the decision process supporting face recognition.</p>
<p>P2: Models of face recognition The face recognition process has been intensely investigated over the last 50 years <span class="citation" data-cites="johnston2009 natu2011">(<a href="#ref-johnston2009" role="doc-biblioref">Johnston and Edmonds 2009</a>; <a href="#ref-natu2011" role="doc-biblioref">Natu and O’Toole 2011</a>)</span>. One of the most influential models describing the face recognition process is the functional model of face recognition proposed by <span class="citation" data-cites="bruce1986">Bruce and Young (<a href="#ref-bruce1986" role="doc-biblioref">1986</a>)</span>. This model advances that visual recognition happens when the encoded structural representation of a face matches the stored structural code of the same face. These structural codes are contained in the face recognition units (FRUs) which have access to person identity nodes that contains identity-specific semantic information. Based on neuroimaging findings, researchers have adapted this model to relate its different components/processes to specific brain areas <span class="citation" data-cites="gobbini2006 gobbini2007 haxby2000">(<a href="#ref-gobbini2006" role="doc-biblioref">Gobbini and Haxby 2006</a>, <a href="#ref-gobbini2007" role="doc-biblioref">2007</a>; <a href="#ref-haxby2000" role="doc-biblioref">Haxby, Hoffman, and Gobbini 2000</a>)</span>. This general model of the distributed neural system for familiar face recognition incorporates two distinct systems. The core system, composed by the inferior occipital gyrus (occipital face area, OFA), the inferior fusiform gyrus (fusiform face area, FFA) and the posterior superior temporal sulcus (pSTS), is dedicated to modality-specific visual processing of faces. The core system is involved in the recognition of faces <em>supported by/based on (established on)</em> visual features only. The extended system participates in the encoding of the person knowledge and is mediated by a broader/larger network of brains areas <span class="citation" data-cites="duchaine2015 gobbini2007 ishai2008 kanwisher2011 haxby2000">(<a href="#ref-duchaine2015" role="doc-biblioref">Duchaine and Yovel 2015</a>; <a href="#ref-gobbini2007" role="doc-biblioref">Gobbini and Haxby 2007</a>; <a href="#ref-ishai2008" role="doc-biblioref">Ishai 2008</a>; <a href="#ref-kanwisher2011" role="doc-biblioref">Kanwisher and Barton 2011</a>; <a href="#ref-haxby2000" role="doc-biblioref">Haxby, Hoffman, and Gobbini 2000</a>)</span>. The extended system support the recognition of faces for which prior semantic knowledge have been stored, like famous faces (add ref.) or personally familiar faces (add ref.), by mediating the retrieval of identity-specific information from memory.</p>
<p>P3: familiar and unfamiliar face recognition Previous studies have shown that this distinction/difference between visual familiarity and person knowledge was also reflected on manifest variables (i.e.&nbsp;response times and accuracy) in tasks assessing face recognition ability. • Visual familiarity, often experimentally induced by exposing faces previously of the experimental task, allows for a faster and more accurate matching of these faces compared to novel ones <span class="citation" data-cites="clutterbuck2005 dwyer2009">(<a href="#ref-clutterbuck2005" role="doc-biblioref">Clutterbuck and Johnston 2005</a>; <a href="#ref-dwyer2009" role="doc-biblioref">Dwyer and Vladeanu 2009</a>)</span>.</p>
<p>• Relying on external features for matching unfamiliar faces? =&gt; check refs. In face matching tasks, unfamiliar face matching disproportionately relies on external features of the face, such as hairstyle and head outline and performance is severely impaired when these external features are removed (Bruce et al., 1999; Clutterbuck &amp; Johnston, 2002; Estudillo &amp; Bindemann, 2014; Henderson et al., 2001; Kemp, Caon, Howard, &amp; Brook, 2016; Megreya &amp; Bindemann, 2009).</p>
<p>Required? Not sure… But make the point that for unfamiliar faces rely more on external visual features.</p>
<p>• In face memory tasks: However, during a face memory task, the findings are less coherent/homogenous. Ref where visual fam faces recognition &lt; novel faces recognition</p>
<p>• Person knowledge familiarity: <span class="citation" data-cites="akan2023 schwaninger2002 clutterbuck2005 read jenkins2011 johnston2009">(<a href="#ref-akan2023" role="doc-biblioref">Akan and Benjamin 2023</a>; <a href="#ref-schwaninger2002" role="doc-biblioref">Schwaninger, Lobmaier, and Collishaw 2002</a>; <a href="#ref-clutterbuck2005" role="doc-biblioref">Clutterbuck and Johnston 2005</a>; <a href="#ref-read" role="doc-biblioref">Read, Vokey, and Hammersley, n.d.</a>; <a href="#ref-jenkins2011" role="doc-biblioref">Jenkins and Burton 2011</a>; <a href="#ref-johnston2009" role="doc-biblioref">Johnston and Edmonds 2009</a>)</span></p>
<p>• Other possible refs: (e.g., Cloutier, Kelley, &amp; Heatherton, 2011; Dubois et al., 1999; Gobbini &amp; Haxby, 2006; Guntupalli, Wheeler, &amp; Gobbini, 2017; Kosaka et al., 2003; Natu &amp; O’Toole, 2015; Rossion, Schiltz, Robaye, Pirenne, &amp; Crommelinck, 2001).</p>
<p>P4: lack of computational approach to investigate human social cognition For the last 50 years, research on face perception and face recognition has been singularly dominated by the separate analysis of manifest variables (i.e.&nbsp;response times and accuracy; ref) or a transformation of the accuracy score (d’; ref). The analysis of theses variables has outlined the understanding framework of face perception and face recognition. The ability to recognize a face is defined in terms of response times and accuracy, separately.</p>
<p>• In human neuroscience research + human social cognition: lack of computational approaches <span class="citation" data-cites="parker anintro2024">(<a href="#ref-parker" role="doc-biblioref">Parker and Ramsey, n.d.</a>; <a href="#ref-anintro2024" role="doc-biblioref">Forstmann and Turner 2024</a>)</span></p>
<p>• Why important?</p>
<p>P5: Evidence accumulation models • One approach: evidence-accumulation models (LBA; <span class="citation" data-cites="brown2008">Brown and Heathcote (<a href="#ref-brown2008" role="doc-biblioref">2008</a>)</span>)</p>
<p>• Computational models of decision-making, choice response: description</p>
<p>• Figure 1: LBA figure (I know which one)</p>
<p>• Latent psychological variables of interest: drift rate (evidence of accumulation) and threshold (response caution).</p>
<p>• Past success in investigating psychology (mostly decision-making, ), some applications in human social cognition</p>
<p>• Now: Increased application</p>
<p>P6: Aims and predictions</p>
<p>• Two experiments were conducted to investigate how familiarity affects the decision-making process of face recognition using evidence-accumulation models.</p>
<p>• Non directional hypotheses:</p>
<p>a) Drift rate (accumulation of evidence) will be affected by familiarity</p>
<p>b) Threshold (response caution) will be affected by familiarity</p>
<p>c) Both drift rate and threshold will be affected by familiarity</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>Here we demonstrated that different computational processes support the recognition of different types of face. Whereas the signal-to-noise ratio provided by visual familiarity was poorer compared to novelty in the first experiment, the combination of visual and semantic knowledge about the individual in the second experiment induced a quicker evidence accumulation for familiar faces compared to novel faces. This would suggest that the quality of the information processing varies as a function of the nature of the information. Such inference about the computational mechanisms involved in face recognition could not have been possible based on the commonly used separate analysis of response times and accuracy. Therefore, we suggest that further work uses evidence-accumulation models (EAM) and other cognitive modelling approaches to build a richer computational understanding of face recognition and familiarity effects.</p>
<p>Accumulation of evidence for visually familiar faces was slower compared to novel faces in Experiment 1. This would suggest that when only surface level visual features are available for recognition, the novelty of the information prevails. In the case of short prior exposure to visual stimuli, our findings are in agreement with other studies that reported a better recognition ability for novel faces than for visually familiar faces <span class="citation" data-cites="bindemann2014 read">(<a href="#ref-bindemann2014" role="doc-biblioref">Bindemann, Attard, and Johnston 2014</a>; <a href="#ref-read" role="doc-biblioref">Read, Vokey, and Hammersley, n.d.</a>)</span>. Visually familiar faces are processed in the core network of face recognition <span class="citation" data-cites="bruce1986 gobbini2006">(<a href="#ref-bruce1986" role="doc-biblioref">Bruce and Young 1986</a>; <a href="#ref-gobbini2006" role="doc-biblioref">Gobbini and Haxby 2006</a>)</span>. This core network manages and synthesizes the visual analysis of low-level visual features to the construction of a detailed perceptual representation of the face. The next step requires the activation of face memory representations, giving rise to a feeling of familiarity proportionate to the degree of overlap between the presented face and the memory trace <span class="citation" data-cites="rapcsak2019">(<a href="#ref-rapcsak2019" role="doc-biblioref">Rapcsak 2019</a>)</span>. In the case of short prior visual exposure, these face memory traces might not be stable enough for the recognition process, leading to a slower evidence accumulation towards the “familiar” decision.</p>
<p>- Here, add the novelty effect (old/new references and explanations)</p>
<p>P3: Effects of person identity (visual + semantic) knowledge (Experiment 2) and the extended network of the functional model of face recognition (Bruce &amp; Young, 1986; Haxby et al., 2000). - 1 sentence: main finding: When richer semantic features available, familiarity prevails (faster accumulation of evidence in favor of familiar information vs novel information). - Extended network - Experiment 2: why Experiment 2 would elicit the extended network of face recognition. - Interpretation of our results within the functional model of face recognition framework. - Suggestion: (Landi et al., 2021) They suggest that there may be two pathways of face memory:</p>
<p> Pathway 1: from Anterior Medial Face area (AM) – Anterior Temporal lobe: facilitate the formation of new associations and feeling of familiarity (Learned Faces)  Pathway 2: from Anterior Medial Face area (AM) – Temporal Pole Face area (TP): access to long-term semantic face information (Familiar Faces) - Evidence accumulation faster for familiar faces could reflect the pathway 2 where semantic face information is accessed quicker than for visually learned faces which, without being associated with semantic information, would need further processing into the Anterior Temporal Lobe (pathway 1), leading to a slower accumulation of evidence.</p>
<p>P3.5: Interpretation of drift rate results for Experiment 2: general issue</p>
<p>P4: Constraints on generality (Simons et al., 2017) - Suggestion: visual familiarity effects not specific to faces - Here we used faces as recognition stimuli - Suggestion: experiment 1 effects generalizable to other visual stimuli processing (objects, scenes…) - However, experiment 2 effects more constrained to faces: difficult to imagine a similar semantic knowledge for objects or scenes that we do for faces. Maybe for very specific and personal object but not comparable to the number of faces that we have a rich semantic knowledge about (or something like that).</p>
<p>P5: Limitations We do not exclude that our parameter estimation might have been affected by our restricted number of trials. Previous studies have investigated how a low number of trials have influenced the model fitting, leading to an incorrect parameter estimation (ref from Manke). While our two experimental designs reach the threshold of the required number of trials to use EAM, more trials could have been more appropriate. Our decision of using 90 trials for each condition was motivated by the fact that it is especially challenging to find or create a face pictures dataset that includes a large number of identities. Another limitation to note concerns the use of celebrities’ picture in the second experiment. Indeed, we cannot exclude the fact that the categorization between familiar faces and novel faces might have been facilitated by low visual features from the celebrities’ pictures (e.g.&nbsp;slightly different orientation of the face, different overall picture quality…). While we tried to control for these features as much as possible, this remains a possibility.</p>
<p>P6: Future research To provide more evidence towards the findings of Landi et al.(2021), a joint-modelling approach could be applied/used/exploited (ref). Joint-modelling designates the particularity of modelling two different source of data (e.g.&nbsp;choice-response data and fMRI data) at the same time. This approach could provide precious insights about the relationship between latent variables and brain areas activity, like the temporal lobe face area (TP). Another direction for future studies would be to investigate how variability in the face recognition ability in the general population accounts for these latent variables’ estimation. (develop this a little more, introducing our possible next experiment). - Last point: Familiarity as a continuum and not binary categorisation</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-akan2023" class="csl-entry" role="listitem">
Akan, Melisa, and Aaron S. Benjamin. 2023. <span>“Haven’t i Seen You Before? Conceptual but Not Perceptual Prior Familiarity Enhances Face Recognition Memory.”</span> <em>Journal of Memory and Language</em> 131 (August): 104433. <a href="https://doi.org/10.1016/j.jml.2023.104433">https://doi.org/10.1016/j.jml.2023.104433</a>.
</div>
<div id="ref-bindemann2014" class="csl-entry" role="listitem">
Bindemann, Markus, Janice Attard, and Robert A. Johnston. 2014. <span>“Perceived Ability and Actual Recognition Accuracy for Unfamiliar and Famous Faces.”</span> Edited by Peter Walla. <em>Cogent Psychology</em> 1 (1): 986903. <a href="https://doi.org/10.1080/23311908.2014.986903">https://doi.org/10.1080/23311908.2014.986903</a>.
</div>
<div id="ref-brown2008" class="csl-entry" role="listitem">
Brown, Scott D., and Andrew Heathcote. 2008. <span>“The Simplest Complete Model of Choice Response Time: Linear Ballistic Accumulation.”</span> <em>Cognitive Psychology</em> 57 (3): 153–78. <a href="https://doi.org/10.1016/j.cogpsych.2007.12.002">https://doi.org/10.1016/j.cogpsych.2007.12.002</a>.
</div>
<div id="ref-bruce1986" class="csl-entry" role="listitem">
Bruce, Vicki, and Andy Young. 1986. <span>“Understanding Face Recognition.”</span> <em>British Journal of Psychology</em> 77 (3): 305–27. <a href="https://doi.org/10.1111/j.2044-8295.1986.tb02199.x">https://doi.org/10.1111/j.2044-8295.1986.tb02199.x</a>.
</div>
<div id="ref-clutterbuck2005" class="csl-entry" role="listitem">
Clutterbuck, R., and R. A. Johnston. 2005. <span>“Demonstrating How Unfamiliar Faces Become Familiar Using a Face Matching Task.”</span> <em>European Journal of Cognitive Psychology</em> 17 (1): 97–116. <a href="https://doi.org/10.1080/09541440340000439">https://doi.org/10.1080/09541440340000439</a>.
</div>
<div id="ref-duchaine2015" class="csl-entry" role="listitem">
Duchaine, Brad, and Galit Yovel. 2015. <span>“A Revised Neural Framework for Face Processing.”</span> <em>Annual Review of Vision Science</em> 1 (1): 393–416. <a href="https://doi.org/10.1146/annurev-vision-082114-035518">https://doi.org/10.1146/annurev-vision-082114-035518</a>.
</div>
<div id="ref-dwyer2009" class="csl-entry" role="listitem">
Dwyer, Dominic M., and Matei Vladeanu. 2009. <span>“Perceptual Learning in Face Processing: Comparison Facilitates Face Recognition.”</span> <em>Quarterly Journal of Experimental Psychology</em> 62 (10): 2055–67. <a href="https://doi.org/10.1080/17470210802661736">https://doi.org/10.1080/17470210802661736</a>.
</div>
<div id="ref-anintro2024" class="csl-entry" role="listitem">
Forstmann, Birte U., and Brandon M. Turner, eds. 2024. <em>An Introduction to Model-Based Cognitive Neuroscience</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-45271-0">https://doi.org/10.1007/978-3-031-45271-0</a>.
</div>
<div id="ref-gobbini2006" class="csl-entry" role="listitem">
Gobbini, M. Ida, and James V. Haxby. 2006. <span>“Neural Response to the Visual Familiarity of Faces.”</span> <em>Brain Research Bulletin</em> 71 (1-3): 76–82. <a href="https://doi.org/10.1016/j.brainresbull.2006.08.003">https://doi.org/10.1016/j.brainresbull.2006.08.003</a>.
</div>
<div id="ref-gobbini2007" class="csl-entry" role="listitem">
———. 2007. <span>“Neural Systems for Recognition of Familiar Faces.”</span> <em>Neuropsychologia</em> 45 (1): 32–41. <a href="https://doi.org/10.1016/j.neuropsychologia.2006.04.015">https://doi.org/10.1016/j.neuropsychologia.2006.04.015</a>.
</div>
<div id="ref-haxby2000" class="csl-entry" role="listitem">
Haxby, James V., Elizabeth A. Hoffman, and M.Ida Gobbini. 2000. <span>“The Distributed Human Neural System for Face Perception.”</span> <em>Trends in Cognitive Sciences</em> 4 (6): 223–33. <a href="https://doi.org/10.1016/S1364-6613(00)01482-0">https://doi.org/10.1016/S1364-6613(00)01482-0</a>.
</div>
<div id="ref-ishai2008" class="csl-entry" role="listitem">
Ishai, Alumit. 2008. <span>“Let’s Face It: It’s a Cortical Network.”</span> <em>NeuroImage</em> 40 (2): 415–19. <a href="https://doi.org/10.1016/j.neuroimage.2007.10.040">https://doi.org/10.1016/j.neuroimage.2007.10.040</a>.
</div>
<div id="ref-jenkins2011" class="csl-entry" role="listitem">
Jenkins, Rob, and A. Mike Burton. 2011. <span>“Stable Face Representations.”</span> <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em> 366 (1571): 1671–83. <a href="https://doi.org/10.1098/rstb.2010.0379">https://doi.org/10.1098/rstb.2010.0379</a>.
</div>
<div id="ref-johnston2009" class="csl-entry" role="listitem">
Johnston, Robert A., and Andrew J. Edmonds. 2009. <span>“Familiar and Unfamiliar Face Recognition: A Review.”</span> <em>Memory</em> 17 (5): 577–96. <a href="https://doi.org/10.1080/09658210902976969">https://doi.org/10.1080/09658210902976969</a>.
</div>
<div id="ref-kanwisher2011" class="csl-entry" role="listitem">
Kanwisher, Nancy, and Jason J. S. Barton. 2011. <span>“111 the Functional Architecture of the Face System: Integrating Evidence from fMRI and Patient Studies.”</span> In <em>Oxford Handbook of Face Perception</em>, edited by Andrew J. Calder, Gillian Rhodes, Mark H. Johnson, and James V. Haxby, 0. Oxford University Press. <a href="https://doi.org/10.1093/oxfordhb/9780199559053.013.0007">https://doi.org/10.1093/oxfordhb/9780199559053.013.0007</a>.
</div>
<div id="ref-natu2011" class="csl-entry" role="listitem">
Natu, Vaidehi, and Alice J. O’Toole. 2011. <span>“The Neural Processing of Familiar and Unfamiliar Faces: A Review and Synopsis.”</span> <em>British Journal of Psychology</em> 102 (4): 726–47. <a href="https://doi.org/10.1111/j.2044-8295.2011.02053.x">https://doi.org/10.1111/j.2044-8295.2011.02053.x</a>.
</div>
<div id="ref-parker" class="csl-entry" role="listitem">
Parker, Samantha, and Richard Ramsey. n.d. <span>“What Can Evidence Accumulation Modelling Tell Us about Human Social Cognition?”</span> <em>Quarterly Journal of Experimental Psychology</em>.
</div>
<div id="ref-rapcsak2019" class="csl-entry" role="listitem">
Rapcsak, Steven Z. 2019. <span>“Face Recognition.”</span> <em>Current Neurology and Neuroscience Reports</em> 19 (7): 41. <a href="https://doi.org/10.1007/s11910-019-0960-9">https://doi.org/10.1007/s11910-019-0960-9</a>.
</div>
<div id="ref-ratcliff1998" class="csl-entry" role="listitem">
Ratcliff, Roger, and Jeffrey N. Rouder. 1998. <span>“Modeling Response Times for Two-Choice Decisions.”</span> <em>Psychological Science</em> 9 (5): 347–56. <a href="https://doi.org/10.1111/1467-9280.00067">https://doi.org/10.1111/1467-9280.00067</a>.
</div>
<div id="ref-read" class="csl-entry" role="listitem">
Read, J Don, John R Vokey, and Richard Hammersley. n.d. <span>“Changing Photos of Faces: Effects of Exposure Duration and Photo Similarity on Recognition and the Accuracy-Confidence Relationship.”</span>
</div>
<div id="ref-schwaninger2002" class="csl-entry" role="listitem">
Schwaninger, Adrian, Janek S. Lobmaier, and Stephan M. Collishaw. 2002. <span>“Role of Featural and Configural Information in Familiar and Unfamiliar Face Recognition.”</span> In <em>Biologically Motivated Computer Vision</em>, edited by Heinrich H. Bülthoff, Christian Wallraven, Seong-Whan Lee, and Tomaso A. Poggio, 2525:643–50. Springer Berlin Heidelberg. <a href="http://link.springer.com/10.1007/3-540-36181-2_64">http://link.springer.com/10.1007/3-540-36181-2_64</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>